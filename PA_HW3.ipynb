{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAWd+xNzRRamXy1YXUC18y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arushi2601/Predictive_Analytics/blob/main/PA_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U6LXcuEPAUYr"
      },
      "outputs": [],
      "source": [
        "#Step 1: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy import stats\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('Breast_Cancer_dataset.csv')\n",
        "\n",
        "# Separate numeric and categorical columns\n",
        "numeric_data = data.select_dtypes(include=[np.number])\n",
        "nonnumeric_data = data.select_dtypes(exclude=[np.number])\n",
        "\n",
        "# Step 1: Handle Missing Values\n",
        "# Impute numeric columns with median\n",
        "imputer_missing_numeric = SimpleImputer(strategy=\"median\")\n",
        "numeric_data_imputed = pd.DataFrame(imputer_missing_numeric.fit_transform(numeric_data), columns=numeric_data.columns)\n",
        "\n",
        "# Impute categorical columns with most frequent value\n",
        "imputer_missing_nonnumeric = SimpleImputer(strategy=\"most_frequent\")\n",
        "nonnumeric_data_imputed = pd.DataFrame(imputer_missing_nonnumeric.fit_transform(nonnumeric_data), columns=nonnumeric_data.columns)\n",
        "\n",
        "\n",
        "for column in nonnumeric_data_imputed.columns:\n",
        "    nonnumeric_data_imputed[column] = LabelEncoder().fit_transform(nonnumeric_data_imputed[column])\n",
        "\n",
        "# Merge numeric and categorical data back\n",
        "data_imputed = pd.concat([numeric_data_imputed, nonnumeric_data_imputed], axis=1)\n",
        "\n",
        "# Step 2: Outlier Detection and Removal\n",
        "z_scores = np.abs(stats.zscore(data_imputed.select_dtypes(include=[np.number])))\n",
        "threshold = 3\n",
        "data_no_outliers = data_imputed[(z_scores < threshold).all(axis=1)]\n",
        "\n",
        "# Step 3: Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "data_standardized = pd.DataFrame(scaler.fit_transform(data_no_outliers), columns=data_no_outliers.columns)\n",
        "\n",
        "# Step 4: Dimensionality Reduction\n",
        "pca = PCA(n_components=0.95)\n",
        "data_reduced = pd.DataFrame(pca.fit_transform(data_standardized))\n",
        "\n",
        "# Split data into features and target variable\n",
        "X = data_reduced\n",
        "y = data_no_outliers['Survival Months']\n"
      ],
      "metadata": {
        "id": "oIHxQQpfAYpG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Performing feature selection\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load the dataset\n",
        "#data = pd.read_csv('/content/Breast_Cancer_dataset.csv')\n",
        "\n",
        "# Preprocessing and encoding\n",
        "data_cleaned = data.dropna()  # Drop rows with missing values\n",
        "X = data_cleaned.drop(columns=['Status', 'Survival Months'])\n",
        "y = data_cleaned['Status']\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Feature selection (top 10 features)\n",
        "selector = SelectKBest(f_classif, k=10)\n",
        "X_selected = selector.fit_transform(X_encoded, y_encoded)\n",
        "selected_feature_names = X_encoded.columns[selector.get_support()]\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_encoded, test_size=0.2, random_state=42)\n",
        "model_performance = defaultdict(dict)\n",
        "\n",
        "# 1: K-Nearest Neighbors (KNN)\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for x in X_test:\n",
        "            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "            predictions.append(np.bincount(k_nearest_labels).argmax())\n",
        "        return predictions\n",
        "\n",
        "\n",
        "knn = KNNClassifier(k=3)\n",
        "knn.fit(X_train, y_train)\n",
        "knn_predictions = knn.predict(X_test)\n",
        "model_performance[\"KNN\"][\"accuracy\"] = accuracy_score(y_test, knn_predictions)\n",
        "model_performance[\"KNN\"][\"classification_report\"] = classification_report(y_test, knn_predictions, target_names=label_encoder.classes_)\n",
        "#Pros: Easy to understand and implement; good for smaller datasets or those with simple structures.\n",
        "#Cons: Computationally expensive on large datasets; sensitive to feature scaling and irrelevant features.\n",
        "#Main Hyperparameters:\n",
        "#k: Number of nearest neighbors to consider (typically chosen through cross-validation).\n",
        "#distance_metric: Often Euclidean, though Manhattan distance is also common.\n",
        "\n",
        "# 2: Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "nb_predictions = nb.predict(X_test)\n",
        "model_performance[\"Naive Bayes\"][\"accuracy\"] = accuracy_score(y_test, nb_predictions)\n",
        "model_performance[\"Naive Bayes\"][\"classification_report\"] = classification_report(y_test, nb_predictions, target_names=label_encoder.classes_)\n",
        "\n",
        "#Pros: Fast, efficient, and performs well on smaller datasets; works well with high-dimensional data.\n",
        "#Cons: Assumes feature independence, which is rarely true in practice; less effective with complex data relationships.\n",
        "#Main Hyperparameters:\n",
        "#var_smoothing: A parameter to add smoothing to variances, preventing overfitting to minor variations in data.\n",
        "\n",
        "# 3: C4.5 Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_predictions = dt.predict(X_test)\n",
        "model_performance[\"Decision Tree\"][\"accuracy\"] = accuracy_score(y_test, dt_predictions)\n",
        "model_performance[\"Decision Tree\"][\"classification_report\"] = classification_report(y_test, dt_predictions, target_names=label_encoder.classes_)\n",
        "#Pros: Simple to interpret, especially with small trees; captures non-linear relationships well.\n",
        "#Cons: Prone to overfitting without pruning; sensitive to small changes in data.\n",
        "#Main Hyperparameters:\n",
        "#max_depth: Maximum depth of the tree (limits tree size to prevent overfitting).\n",
        "#min_samples_split: Minimum number of samples required to split a node.\n",
        "#criterion: Function to measure split quality (e.g., Gini or entropy).\n",
        "\n",
        "\n",
        "# 4: Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_predictions = rf.predict(X_test)\n",
        "model_performance[\"Random Forest\"][\"accuracy\"] = accuracy_score(y_test, rf_predictions)\n",
        "model_performance[\"Random Forest\"][\"classification_report\"] = classification_report(y_test, rf_predictions, target_names=label_encoder.classes_)\n",
        "#Pros: Resistant to overfitting; can capture complex interactions between features.\n",
        "#Cons: Less interpretable due to the large number of trees; slower training and prediction times compared to single models.\n",
        "#Main Hyperparameters:\n",
        "#n_estimators: Number of decision trees in the ensemble.\n",
        "#max_depth: Maximum depth of each individual tree.\n",
        "#min_samples_split: Minimum samples required to split an internal node.\n",
        "\n",
        "# 5: Gradient Boosting\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "gb_predictions = gb.predict(X_test)\n",
        "model_performance[\"Gradient Boosting\"][\"accuracy\"] = accuracy_score(y_test, gb_predictions)\n",
        "model_performance[\"Gradient Boosting\"][\"classification_report\"] = classification_report(y_test, gb_predictions, target_names=label_encoder.classes_)\n",
        "#Pros: High performance, particularly on structured/tabular data; capable of capturing complex relationships.\n",
        "#Cons: Prone to overfitting with insufficient regularization; requires careful tuning of hyperparameters.\n",
        "#Main Hyperparameters:\n",
        "#n_estimators: Number of trees added to the model (more trees generally improve performance up to a point).\n",
        "#learning_rate: Step size for each iteration (smaller values provide better generalization).\n",
        "#max_depth: Limits depth of each tree, reducing model complexity.\n",
        "\n",
        "# Display the model performance\n",
        "for model_name, metrics in model_performance.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']}\")\n",
        "    print(f\"Classification Report:\\n{metrics['classification_report']}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkSN1KZpFPLD",
        "outputId": "f89f71dc-14b8-4282-c28a-dae09852426d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: KNN\n",
            "Accuracy: 0.8253275109170306\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Alive       0.85      0.96      0.90       385\n",
            "        Dead       0.37      0.14      0.20        73\n",
            "\n",
            "    accuracy                           0.83       458\n",
            "   macro avg       0.61      0.55      0.55       458\n",
            "weighted avg       0.78      0.83      0.79       458\n",
            "\n",
            "\n",
            "Model: Naive Bayes\n",
            "Accuracy: 0.8187772925764192\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Alive       0.90      0.89      0.89       385\n",
            "        Dead       0.43      0.45      0.44        73\n",
            "\n",
            "    accuracy                           0.82       458\n",
            "   macro avg       0.66      0.67      0.67       458\n",
            "weighted avg       0.82      0.82      0.82       458\n",
            "\n",
            "\n",
            "Model: Decision Tree\n",
            "Accuracy: 0.7969432314410481\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Alive       0.86      0.90      0.88       385\n",
            "        Dead       0.31      0.23      0.27        73\n",
            "\n",
            "    accuracy                           0.80       458\n",
            "   macro avg       0.59      0.57      0.57       458\n",
            "weighted avg       0.77      0.80      0.78       458\n",
            "\n",
            "\n",
            "Model: Random Forest\n",
            "Accuracy: 0.8078602620087336\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Alive       0.85      0.93      0.89       385\n",
            "        Dead       0.31      0.16      0.21        73\n",
            "\n",
            "    accuracy                           0.81       458\n",
            "   macro avg       0.58      0.55      0.55       458\n",
            "weighted avg       0.77      0.81      0.78       458\n",
            "\n",
            "\n",
            "Model: Gradient Boosting\n",
            "Accuracy: 0.8580786026200873\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Alive       0.86      0.99      0.92       385\n",
            "        Dead       0.79      0.15      0.25        73\n",
            "\n",
            "    accuracy                           0.86       458\n",
            "   macro avg       0.82      0.57      0.59       458\n",
            "weighted avg       0.85      0.86      0.82       458\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Hyperparamter tuning using Random forest and Gradient boosting\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grids for each model\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Random Forest Hyperparameter Tuning\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf_grid_search = GridSearchCV(estimator=rf, param_grid=rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters for Random Forest\n",
        "rf_best_params = rf_grid_search.best_params_\n",
        "rf_best_score = rf_grid_search.best_score_\n",
        "\n",
        "# Gradient Boosting Hyperparameter Tuning\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb_grid_search = GridSearchCV(estimator=gb, param_grid=gb_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "gb_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters for Gradient Boosting\n",
        "gb_best_params = gb_grid_search.best_params_\n",
        "gb_best_score = gb_grid_search.best_score_\n",
        "\n",
        "# Print the best parameters and scores for each model\n",
        "print(\"Best Random Forest Parameters:\", rf_best_params)\n",
        "print(\"Best Random Forest Cross-Validation Accuracy:\", rf_best_score)\n",
        "\n",
        "print(\"Best Gradient Boosting Parameters:\", gb_best_params)\n",
        "print(\"Best Gradient Boosting Cross-Validation Accuracy:\", gb_best_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0awOy66SK3k3",
        "outputId": "71a2a4a9-31e9-4dac-8561-f5c36f682a03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Best Random Forest Cross-Validation Accuracy: 0.8476228763717039\n",
            "Best Gradient Boosting Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}\n",
            "Best Gradient Boosting Cross-Validation Accuracy: 0.8443486547252126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Results\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# Retrain models with best hyperparameters from step 3\n",
        "# Random Forest with best parameters\n",
        "best_rf = RandomForestClassifier(**rf_best_params, random_state=42)\n",
        "best_rf.fit(X_train, y_train)\n",
        "rf_best_predictions = best_rf.predict(X_test)\n",
        "rf_best_accuracy = accuracy_score(y_test, rf_best_predictions)\n",
        "rf_conf_matrix = confusion_matrix(y_test, rf_best_predictions)\n",
        "rf_class_report = classification_report(y_test, rf_best_predictions, target_names=label_encoder.classes_)\n",
        "\n",
        "# Gradient Boosting with best parameters\n",
        "best_gb = GradientBoostingClassifier(**gb_best_params, random_state=42)\n",
        "best_gb.fit(X_train, y_train)\n",
        "gb_best_predictions = best_gb.predict(X_test)\n",
        "gb_best_accuracy = accuracy_score(y_test, gb_best_predictions)\n",
        "gb_conf_matrix = confusion_matrix(y_test, gb_best_predictions)\n",
        "gb_class_report = classification_report(y_test, gb_best_predictions, target_names=label_encoder.classes_)\n",
        "\n",
        "print(\"\\nFinal Model Performance Summary:\")\n",
        "print(f\"Random Forest Best Accuracy: {rf_best_accuracy}\")\n",
        "print(f\"Gradient Boosting Best Accuracy: {gb_best_accuracy}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix for Random Forest:\")\n",
        "print(rf_conf_matrix)\n",
        "print(\"\\nClassification Report for Random Forest:\")\n",
        "print(rf_class_report)\n",
        "\n",
        "print(\"\\nConfusion Matrix for Gradient Boosting:\")\n",
        "print(gb_conf_matrix)\n",
        "print(\"\\nClassification Report for Gradient Boosting:\")\n",
        "print(gb_class_report)\n",
        "\n",
        "print(\"\\nImportant features used from Random Forest:\")\n",
        "feature_importances_rf = pd.Series(best_rf.feature_importances_, index=selected_feature_names).sort_values(ascending=False)\n",
        "print(feature_importances_rf)\n",
        "\n",
        "print(\"\\nImportant Features used from Gradient Boosting:\")\n",
        "feature_importances_gb = pd.Series(best_gb.feature_importances_, index=selected_feature_names).sort_values(ascending=False)\n",
        "print(feature_importances_gb)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIEQG9JqTGx1",
        "outputId": "0f4c45a2-79f8-40a4-d921-bc711d45e4e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Model Performance Summary:\n",
            "Random Forest Best Accuracy: 0.8384279475982532\n",
            "Gradient Boosting Best Accuracy: 0.851528384279476\n",
            "\n",
            "Confusion Matrix for Random Forest:\n",
            "[[374  11]\n",
            " [ 63  10]]\n",
            "\n",
            "Classification Report for Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Alive       0.86      0.97      0.91       385\n",
            "        Dead       0.48      0.14      0.21        73\n",
            "\n",
            "    accuracy                           0.84       458\n",
            "   macro avg       0.67      0.55      0.56       458\n",
            "weighted avg       0.80      0.84      0.80       458\n",
            "\n",
            "\n",
            "Confusion Matrix for Gradient Boosting:\n",
            "[[384   1]\n",
            " [ 67   6]]\n",
            "\n",
            "Classification Report for Gradient Boosting:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Alive       0.85      1.00      0.92       385\n",
            "        Dead       0.86      0.08      0.15        73\n",
            "\n",
            "    accuracy                           0.85       458\n",
            "   macro avg       0.85      0.54      0.53       458\n",
            "weighted avg       0.85      0.85      0.80       458\n",
            "\n",
            "\n",
            "Important features used from Random Forest:\n",
            "Tumor Size                             0.384392\n",
            "Reginol Node Positive                  0.301384\n",
            "Progesterone Status_Positive           0.070339\n",
            "Estrogen Status_Positive               0.053149\n",
            "N Stage_N3                             0.046897\n",
            "6th Stage_IIIC                         0.042549\n",
            "Grade_2                                0.032032\n",
            "A Stage_Regional                       0.026557\n",
            "differentiate_Poorly differentiated    0.021601\n",
            "Grade_3                                0.021098\n",
            "dtype: float64\n",
            "\n",
            "Important Features used from Gradient Boosting:\n",
            "Reginol Node Positive                  0.509649\n",
            "Tumor Size                             0.209415\n",
            "Progesterone Status_Positive           0.141712\n",
            "Estrogen Status_Positive               0.061356\n",
            "A Stage_Regional                       0.021480\n",
            "differentiate_Poorly differentiated    0.014863\n",
            "Grade_3                                0.014451\n",
            "Grade_2                                0.012912\n",
            "6th Stage_IIIC                         0.008521\n",
            "N Stage_N3                             0.005640\n",
            "dtype: float64\n"
          ]
        }
      ]
    }
  ]
}